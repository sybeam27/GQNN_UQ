{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import wandb\n",
    "import pickle\n",
    "import sys \n",
    "from utills.function import generate_graph_data, generate_noisy_graph_data, load_county_graph_data, load_twitch_graph_data, load_wiki_graph_data, load_trans_graph_data, create_ba_graph_pyg, create_er_graph_pyg, create_grid_graph_pyg, create_tree_graph_pyg\n",
    "from torch_geometric.logging import log\n",
    "from torch_geometric.data import Data\n",
    "from scipy.stats import pearsonr\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from conformalized_gnn.model import GNN, ConfGNN, ConfMLP\n",
    "from conformalized_gnn.calibrator import TS, VS, ETS, CaGCN, GATS\n",
    "from conformalized_gnn.conformal import run_conformal_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'basic'\n",
    "nodes = 1000\n",
    "noise = 0.3\n",
    "hidden_channels = 64\n",
    "model = 'GCN'\n",
    "heads = 1\n",
    "aggr = 'sum'\n",
    "alpha = 0.1\n",
    "lr = 1e-3\n",
    "epochs = 500\n",
    "device = 'cuda:0'\n",
    "conformal_score = 'cqr'\n",
    "conf_correct_model = 'gnn'\n",
    "quantile = False\n",
    "bnn = False\n",
    "target_size = 0\n",
    "confnn_hidden_dim = 64\n",
    "confgnn_num_layers = 1\n",
    "confgnn_base_model = 'GCN'\n",
    "confgnn_lr = 1e-3\n",
    "tau = 0.1\n",
    "size_loss_weight = 1\n",
    "reg_loss_weight = 1\n",
    "not_save_res = False\n",
    "num_runs = 10\n",
    "retrain = False\n",
    "data_seed = 0\n",
    "cond_cov_loss = False\n",
    "calib_fraction = 0.5\n",
    "task = 'regression'\n",
    "metric = 'eff_valid_cqr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conformal_score != 'cqr':\n",
    "    raise ValueError('For regression task, the training conformal score should be cqr!')\n",
    "\n",
    "if bnn:\n",
    "    quantile = False\n",
    "else:    \n",
    "    quantile = True \n",
    "    \n",
    "device = torch.device(device)\n",
    "\n",
    "name = dataset + '_' + model\n",
    "if alpha != 0.1:\n",
    "    name += '_alpha_' + str(alpha)    \n",
    "if bnn:\n",
    "    name += '_bnn'\n",
    " \n",
    "if dataset == 'basic':\n",
    "    graph_data = generate_graph_data(num_nodes=nodes)\n",
    "elif dataset in ('gaussian', 'uniform', 'outlier', 'edge'):\n",
    "    graph_data = generate_noisy_graph_data(num_nodes=nodes, noise_type=dataset, noise_level=noise)\n",
    "elif dataset in ('education', 'election', 'income', 'unemployment'):\n",
    "    graph_data = load_county_graph_data(dataset, 2012)\n",
    "elif dataset in ('DE', 'ENGB', 'ES', 'FR', 'PTBR', 'RU'):\n",
    "    graph_data = load_twitch_graph_data(dataset)\n",
    "elif dataset in ('chameleon', 'crocodile', 'squirrel'):\n",
    "    graph_data = load_wiki_graph_data(dataset)\n",
    "elif dataset in ('Anaheim', 'ChicagoSketch'):\n",
    "    graph_data = load_trans_graph_data(dataset)\n",
    "elif dataset == 'BA':\n",
    "    graph_data = create_ba_graph_pyg(n=nodes)\n",
    "elif dataset == 'ER':\n",
    "    graph_data = create_er_graph_pyg(n=nodes)\n",
    "elif dataset == 'grid':\n",
    "    graph_data = create_grid_graph_pyg()\n",
    "elif dataset == 'tree':\n",
    "    graph_data = create_tree_graph_pyg()\n",
    "\n",
    "def gaussian_nll_loss(mean, log_var, y_true):\n",
    "    # Compute the negative log likelihood for a Gaussian distribution\n",
    "    precision = torch.exp(-log_var)\n",
    "    mse_loss = F.mse_loss(mean, y_true, reduction='none')\n",
    "    nll_loss = 0.5 * (mse_loss * precision + log_var + torch.log(torch.tensor(2 * np.pi)))\n",
    "    return torch.mean(nll_loss)\n",
    "\n",
    "def train(epoch, model, data, optimizer, alpha):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "    if quantile:\n",
    "        mid = out[:, 0][data.train_mask].reshape(-1,1)\n",
    "        label = data.y[data.train_mask].reshape(-1,1)\n",
    "        mse_loss = F.mse_loss(mid, label)\n",
    "        low_bound = alpha/2\n",
    "        upp_bound = 1 - alpha/2\n",
    "        lower = out[:, 1][data.train_mask].reshape(-1,1)\n",
    "        upper = out[:, 2][data.train_mask].reshape(-1,1)\n",
    "        low_loss = torch.mean(torch.max((low_bound - 1) * (label - lower), low_bound * (label - lower)))\n",
    "        upp_loss = torch.mean(torch.max((upp_bound - 1) * (label - upper), upp_bound * (label - upper)))\n",
    "        loss = mse_loss + low_loss + upp_loss\n",
    "    elif bnn:\n",
    "        mu = out[:, 0][data.train_mask].reshape(-1,1)\n",
    "        logvar = out[:, 1][data.train_mask].reshape(-1,1)\n",
    "        loss = gaussian_nll_loss(mu, logvar, data.y[data.train_mask])\n",
    "    else:\n",
    "        loss = F.mse_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if quantile:\n",
    "        return (float(loss), mse_loss, low_loss, upp_loss)\n",
    "    elif bnn:\n",
    "        return float(loss)\n",
    "    else:\n",
    "        return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, alpha, tau, target_size, size_loss = False):\n",
    "    model.eval()\n",
    "    \n",
    "    if size_loss:\n",
    "        pred_raw, ori_pred_raw = model(data.x, data.edge_index)\n",
    "    else:\n",
    "        pred_raw = model(data.x, data.edge_index)\n",
    "        \n",
    "    if quantile:\n",
    "        pred = pred_raw[:, 0]\n",
    "    elif bnn:\n",
    "        pred = pred_raw[:, 0]\n",
    "    else:\n",
    "        pred = pred_raw\n",
    "        \n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.valid_mask, data.calib_test_mask]:\n",
    "        accs.append(pearsonr(pred[mask].detach().cpu().numpy().reshape(-1), \n",
    "                             data.y[mask].detach().cpu().numpy().reshape(-1))[0])\n",
    "    if size_loss:\n",
    "        if task == 'regression':\n",
    "            if quantile:\n",
    "                query_idx = np.where(data.valid_mask)[0]\n",
    "                np.random.seed(0)\n",
    "                np.random.shuffle(query_idx)\n",
    "\n",
    "                train_train_idx = query_idx[:int(len(query_idx)/2)]\n",
    "                train_calib_idx = query_idx[int(len(query_idx)/2):]\n",
    "                \n",
    "                n_temp = len(train_calib_idx)\n",
    "                ### use only train_train nodes\n",
    "                mid = pred_raw[:, 0][train_calib_idx].reshape(-1,1)\n",
    "                label = data.y[train_calib_idx].reshape(-1,1)\n",
    "                mse_loss = F.mse_loss(mid, label)\n",
    "                low_bound = alpha/2\n",
    "                upp_bound = 1 - alpha/2\n",
    "                lower = pred_raw[:, 1][train_calib_idx].reshape(-1,1)\n",
    "                upper = pred_raw[:, 2][train_calib_idx].reshape(-1,1)\n",
    "                low_loss = torch.mean(torch.max((low_bound - 1) * (label - lower), low_bound * (label - lower)))\n",
    "                upp_loss = torch.mean(torch.max((upp_bound - 1) * (label - upper), upp_bound * (label - upper)))\n",
    "                \n",
    "                ## CQR loss\n",
    "                size_loss = 0\n",
    "                lower_calib = pred_raw[:, 1][train_train_idx].reshape(-1,1)\n",
    "                upper_calib = pred_raw[:, 2][train_train_idx].reshape(-1,1)\n",
    "                label_calib = data.y[train_train_idx].reshape(-1,1)\n",
    "\n",
    "                cal_scores = torch.maximum(label_calib-upper_calib, lower_calib-label_calib)\n",
    "                # Get the score quantile\n",
    "                qhat = torch.quantile(cal_scores, np.ceil((n_temp+1)*(1-alpha))/n_temp, interpolation='higher')\n",
    "                size_loss = torch.mean(upper_calib + qhat - (lower_calib - qhat))\n",
    "                pred_loss = mse_loss + low_loss + upp_loss\n",
    "        elif bnn:\n",
    "            raise ValueError('Not implemented....')\n",
    "        else:\n",
    "            out_softmax = F.softmax(pred_raw, dim = 1)\n",
    "            query_idx = np.where(data.valid_mask)[0]\n",
    "            np.random.seed(0)\n",
    "            np.random.shuffle(query_idx)\n",
    "\n",
    "            train_train_idx = query_idx[:int(len(query_idx)/2)]\n",
    "            train_calib_idx = query_idx[int(len(query_idx)/2):]\n",
    "\n",
    "            n_temp = len(train_calib_idx)\n",
    "            q_level = np.ceil((n_temp+1)*(1-alpha))/n_temp\n",
    "\n",
    "            tps_conformal_score = out_softmax[train_calib_idx][torch.arange(len(train_calib_idx)), data.y[train_calib_idx]]\n",
    "            qhat = torch.quantile(tps_conformal_score, 1 - q_level, interpolation='higher')\n",
    "            c = torch.sigmoid((out_softmax[train_train_idx] - qhat)/tau)\n",
    "            size_loss = torch.mean(torch.relu(torch.sum(c, axis = 1) - target_size))\n",
    "            \n",
    "        return accs, pred_raw, size_loss.item()\n",
    "    else:\n",
    "        return accs, pred_raw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=graph_data.x, edge_index=graph_data.edge_index, y=graph_data.y)\n",
    "x = data.x\n",
    "y = data.y\n",
    "\n",
    "idx = np.array(range(len(y)))  \n",
    "np.random.seed(data_seed)\n",
    "np.random.shuffle(idx)\n",
    "split_res = np.split(idx, [int(0.5 * len(idx)), int(0.6 * len(idx)), len(idx)])\n",
    "train_idx, valid, calib_test = split_res[0], split_res[1], split_res[2]\n",
    "\n",
    "data.train_mask = np.array([False] * len(y)) \n",
    "data.train_mask[train_idx] = True\n",
    "\n",
    "data.valid_mask = np.array([False] * len(y)) \n",
    "data.valid_mask[valid] = True\n",
    "\n",
    "data.calib_test_mask = np.array([False] * len(y)) \n",
    "data.calib_test_mask[calib_test] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 100\n",
    "n = min(1000, int(calib_test.shape[0]/2))\n",
    "alpha = alpha\n",
    "tau = tau\n",
    "target_size = target_size\n",
    "num_conf_layers = confgnn_num_layers\n",
    "base_model = confgnn_base_model\n",
    "optimal_examine_res = {}\n",
    "tau2res = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training base model from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m             best_pred \u001b[38;5;241m=\u001b[39m pred\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# torch.save(best_model, model_checkpoint)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mbest_pred\u001b[49m\n\u001b[1;32m     60\u001b[0m (train_acc, val_acc, test_acc), _ \u001b[38;5;241m=\u001b[39m test(best_model, data, alpha, tau, target_size, size_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m result_this_run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgnn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_pred' is not defined"
     ]
    }
   ],
   "source": [
    "for run in tqdm(range(num_runs)):\n",
    "    result_this_run = {}\n",
    "    \n",
    "    if quantile:\n",
    "        if alpha == 0.1:\n",
    "            model_checkpoint = './model/' + model + '_' + dataset + '_' + str(run+1) + '_quantile_0410.pt'\n",
    "        else:\n",
    "            model_checkpoint = './model/' + model + '_' + dataset + '_' + str(run+1) + '_quantile_' + str(alpha) + '_0410.pt'\n",
    "    elif bnn:\n",
    "        model_checkpoint = './model/' + model + '_' + dataset + '_' + str(run+1) + '_bnn_' + str(alpha) + '_0410.pt'\n",
    "    else:\n",
    "        model_checkpoint = './model/' + model + '_' + dataset + '_' + str(run+1) + '_0410.pt'\n",
    "        \n",
    "    if quantile:\n",
    "        output_dim = 3\n",
    "    elif bnn:\n",
    "        output_dim = 2\n",
    "    else:\n",
    "        output_dim = 1\n",
    "    num_features = x.shape[1]\n",
    "    \n",
    "    if (os.path.exists(model_checkpoint)) and (not retrain):\n",
    "        print('loading saved base model...')\n",
    "        model = torch.load(model_checkpoint, map_location = device)\n",
    "        model, data = model.to(device), data.to(device)\n",
    "        model.eval()\n",
    "        pred = model(data.x, data.edge_index)\n",
    "        best_model = model\n",
    "        best_pred = pred\n",
    "    else:\n",
    "        print('training base model from scratch...')\n",
    "        model = GNN(num_features, hidden_channels, output_dim, model, heads, aggr)    \n",
    "\n",
    "        model, data = model.to(device), data.to(device)\n",
    "        optimizer = torch.optim.Adam([\n",
    "            dict(params=model.conv1.parameters(), weight_decay=5e-4),\n",
    "            dict(params=model.conv2.parameters(), weight_decay=0)\n",
    "        ], lr=lr)  # Only perform weight-decay on first convolution.\n",
    "\n",
    "        best_val_acc = final_test_acc = 0\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            loss = train(epoch, model, data, optimizer, alpha)\n",
    "            if quantile:\n",
    "                mse = loss[1]\n",
    "                lower = loss[2]\n",
    "                upper = loss[3]\n",
    "                loss = loss[0]\n",
    "            \n",
    "            (train_acc, val_acc, tmp_test_calib_acc), pred = test(model, data, alpha, tau, target_size)\n",
    "            if val_acc > best_val_acc:\n",
    "                #torch.save(best_model, model_checkpoint)\n",
    "                best_model = copy.deepcopy(model)\n",
    "                best_val_acc = val_acc\n",
    "                test_acc = tmp_test_calib_acc\n",
    "                best_pred = pred\n",
    "            \n",
    "        # torch.save(best_model, model_checkpoint)\n",
    "        pred = best_pred\n",
    "\n",
    "    (train_acc, val_acc, test_acc), _ = test(best_model, data, alpha, tau, target_size, size_loss = False)\n",
    "        \n",
    "    result_this_run['gnn'] = {}\n",
    "    result_this_run['gnn']['CQR'] = run_conformal_regression(pred, data, n, alpha, calib_eval = False)\n",
    "    \n",
    "    condcov_epochs = []\n",
    "    result_this_run['conf_gnn'] = {}\n",
    "    if bnn:\n",
    "        result_this_run['conf_gnn']['Raw'] = run_conformal_regression(pred, data, n, alpha, score = 'qr', calib_eval = False)        \n",
    "    else:    \n",
    "        model_to_correct = copy.deepcopy(model)\n",
    "        if conf_correct_model == 'gnn':\n",
    "            confmodel = ConfGNN(model_to_correct, data, args, num_conf_layers, base_model, output_dim, task).to(device)\n",
    "        elif conf_correct_model == 'mlp':\n",
    "            confmodel = ConfMLP(model_to_correct, data, output_dim, task).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(confmodel.parameters(), weight_decay=5e-4, lr=confgnn_lr)  # Only perform weight-decay on first convolution.\n",
    "        pred_loss_hist, size_loss_hist, cons_loss_hist, val_size_loss_hist = [], [], [], []\n",
    "        best_size_loss = 10000\n",
    "        best_val_acc = 0\n",
    "        \n",
    "        print('Starting topology-aware conformal correction...')\n",
    "        for epoch in range(1, epochs + 1):  \n",
    "            confmodel.train()\n",
    "            optimizer.zero_grad()\n",
    "            out, ori_out = confmodel(data.x, data.edge_index)\n",
    "            \n",
    "            if task == 'regression':\n",
    "                if quantile:\n",
    "                    ### use only train_train nodes\n",
    "                    mid = out[:, 0][train_train_idx].reshape(-1,1)\n",
    "                    label = data.y[train_train_idx].reshape(-1,1)\n",
    "                    mse_loss = F.mse_loss(mid, label)\n",
    "                    low_bound = alpha/2\n",
    "                    upp_bound = 1 - alpha/2\n",
    "                    lower = out[:, 1][train_train_idx].reshape(-1,1)\n",
    "                    upper = out[:, 2][train_train_idx].reshape(-1,1)\n",
    "                    low_loss = torch.mean(torch.max((low_bound - 1) * (label - lower), low_bound * (label - lower)))\n",
    "                    upp_loss = torch.mean(torch.max((upp_bound - 1) * (label - upper), upp_bound * (label - upper)))\n",
    "                    pred_loss = mse_loss + low_loss + upp_loss\n",
    "\n",
    "                    n_temp = len(train_calib_idx)\n",
    "                    ## CQR loss\n",
    "                    lower_calib = out[:, 1][train_calib_idx].reshape(-1,1)\n",
    "                    upper_calib = out[:, 2][train_calib_idx].reshape(-1,1)\n",
    "                    label_calib = data.y[train_calib_idx].reshape(-1,1)\n",
    "\n",
    "                    cal_scores = torch.maximum(label_calib-upper_calib, lower_calib-label_calib)\n",
    "                    # Get the score quantile\n",
    "                    qhat = torch.quantile(cal_scores, np.ceil((n_temp+1)*(1-alpha))/n_temp, interpolation='higher')\n",
    "\n",
    "                    lower_test = out[:, 1][train_test_idx].reshape(-1,1)\n",
    "                    upper_test = out[:, 2][train_test_idx].reshape(-1,1)\n",
    "                    \n",
    "                    lower_deviate_loss = F.mse_loss(out[:, 1].reshape(-1,1), ori_out[:, 1].reshape(-1,1))\n",
    "                    upper_deviate_loss = F.mse_loss(out[:, 2].reshape(-1,1), ori_out[:, 2].reshape(-1,1))\n",
    "                                            \n",
    "                    size_loss = torch.mean(upper_test + qhat - (lower_test - qhat))\n",
    "                    \n",
    "                    loss = pred_loss\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    loss = float(loss)\n",
    "                    pred_loss_hist.append(pred_loss.item())\n",
    "                    size_loss_hist.append(size_loss.item())\n",
    "                    \n",
    "                    (train_acc, val_acc, tmp_test_calib_acc), pred, size_loss = test(confmodel, data, alpha, tau, target_size, size_loss = True)\n",
    "            \n",
    "                    eff_valid = run_conformal_regression(pred, data, n, alpha, validation_set = True)[1]\n",
    "                    \n",
    "                    val_size_loss_hist.append(size_loss)\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        test_acc = tmp_test_calib_acc\n",
    "                        best_pred = pred  \n",
    "                    \n",
    "        result_this_run['conf_gnn'] = {}\n",
    "        result_this_run['conf_gnn']['CQR'] = run_conformal_regression(best_pred, data, n, alpha, calib_eval = conftr_calib_holdout, calib_fraction = calib_fraction)\n",
    "        result_this_run['conf_gnn']['eff_valid'] = run_conformal_regression(best_pred, data, n, alpha, validation_set = True)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tau2res[run] = result_this_run\n",
    "print('Finished training this run!')\n",
    "\n",
    "if not os.path.exists('./pred'):\n",
    "    os.mkdir('./pred')\n",
    "if not not_save_res:\n",
    "    print('Saving results to', './pred/' + name +'.pkl')\n",
    "    with open('./pred/' + name +'.pkl', 'wb') as f:\n",
    "        pickle.dump(tau2res, f)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saa_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
